{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 160M/160M [00:01<00:00, 162MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved boxed image to /content/PXL_20230909_230220242_boxed.jpg\n",
      "Centers for PXL_20230909_230220242.jpg: [(0.17665913701057434, 0.6796562671661377), (0.49221178889274597, 0.6882927417755127), (0.7483956217765808, 0.6953920722007751), (0.8390349745750427, 0.6998414993286133), (0.39303773641586304, 0.686179518699646), (0.6381164789199829, 0.6930094957351685), (0.3017145097255707, 0.6844546794891357)]\n",
      "No subjects detected in /content/PXL_20250308_051545003.jpg\n",
      "Saved boxed image to /content/PXL_20240123_074008546_boxed.jpg\n",
      "Centers for PXL_20240123_074008546.jpg: [(0.865150511264801, 0.5346399545669556), (0.7489351630210876, 0.5015894174575806), (0.7444515824317932, 0.4990505576133728), (0.7415317296981812, 0.49647387862205505), (0.7453347444534302, 0.5042505264282227)]\n",
      "\n",
      "Overall patterns across 3 images:\n",
      "- Average subject center: x=0.62 (0=left, 1=right), y=0.61 (0=top, 1=bottom)\n",
      "- Tendency to place subjects on the right.\n",
      "- Subjects often in the lower part of the frame.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.io import read_image\n",
    "from torchvision.utils import draw_bounding_boxes, save_image\n",
    "from PIL import Image  # For optional thumbnail or display if needed\n",
    "\n",
    "# Load pre-trained object detection model\n",
    "model = fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")  # Downloads weights on first run\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "# Transformation for input (model expects normalized tensors)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "def analyze_image(image_path, confidence_threshold=0.5):\n",
    "    # Load image\n",
    "    image = read_image(image_path)\n",
    "    image_tensor = transform(Image.open(image_path).convert(\"RGB\"))\n",
    "\n",
    "    # Run detection\n",
    "    with torch.no_grad():\n",
    "        predictions = model([image_tensor])[0]\n",
    "\n",
    "    # Filter boxes by confidence\n",
    "    high_conf_idx = predictions['scores'] > confidence_threshold\n",
    "    boxes = predictions['boxes'][high_conf_idx]\n",
    "    labels = predictions['labels'][high_conf_idx]\n",
    "\n",
    "    if len(boxes) == 0:\n",
    "        print(f\"No subjects detected in {image_path}\")\n",
    "        return None\n",
    "\n",
    "    # Draw boxes on image\n",
    "    image_with_boxes = draw_bounding_boxes(image, boxes, width=5, colors=\"red\")\n",
    "\n",
    "    # Save the output\n",
    "    output_path = image_path.replace('.jpg', '_boxed.jpg')  # Adjust extension if needed\n",
    "    save_image(image_with_boxes.float() / 255, output_path)\n",
    "    print(f\"Saved boxed image to {output_path}\")\n",
    "\n",
    "    # Compute positions for patterns (normalized centers)\n",
    "    height, width = image.shape[1], image.shape[2]  # H, W from tensor\n",
    "    centers = []\n",
    "    for box in boxes:\n",
    "        xmin, ymin, xmax, ymax = box\n",
    "        center_x = ((xmin + xmax) / 2) / width\n",
    "        center_y = ((ymin + ymax) / 2) / height\n",
    "        centers.append((center_x.item(), center_y.item()))\n",
    "\n",
    "    return centers\n",
    "\n",
    "# Main function to process a folder\n",
    "def analyze_folder(folder_path):\n",
    "    image_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n",
    "    all_centers = []\n",
    "\n",
    "    for file in image_files:\n",
    "        path = os.path.join(folder_path, file)\n",
    "        centers = analyze_image(path)\n",
    "        if centers:\n",
    "            all_centers.extend(centers)\n",
    "            print(f\"Centers for {file}: {centers}\")\n",
    "\n",
    "    if all_centers:\n",
    "        # Basic pattern analysis (e.g., average position)\n",
    "        avg_x = sum(c[0] for c in all_centers) / len(all_centers)\n",
    "        avg_y = sum(c[1] for c in all_centers) / len(all_centers)\n",
    "        print(f\"\\nOverall patterns across {len(image_files)} images:\")\n",
    "        print(f\"- Average subject center: x={avg_x:.2f} (0=left, 1=right), y={avg_y:.2f} (0=top, 1=bottom)\")\n",
    "        if avg_x < 0.4:\n",
    "            print(\"- Tendency to place subjects on the left.\")\n",
    "        elif avg_x > 0.6:\n",
    "            print(\"- Tendency to place subjects on the right.\")\n",
    "        else:\n",
    "            print(\"- Subjects often centered horizontally.\")\n",
    "        if avg_y < 0.4:\n",
    "            print(\"- Subjects often in the upper part of the frame.\")\n",
    "        elif avg_y > 0.6:\n",
    "            print(\"- Subjects often in the lower part of the frame.\")\n",
    "        else:\n",
    "            print(\"- Subjects often centered vertically.\")\n",
    "\n",
    "folder_path = \"/content/\"  # Change to your folder\n",
    "analyze_folder(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy.ndimage as ndimage\n",
    "from torchvision.utils import draw_bounding_boxes, save_image\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import Resize\n",
    "import torch\n",
    "\n",
    "def get_centroids(small_data, k=8, max_iters=10):\n",
    "    n = small_data.shape[0]\n",
    "    centroids = small_data[torch.randperm(n)[:k]]\n",
    "    for _ in range(max_iters):\n",
    "        dist = torch.cdist(small_data, centroids)\n",
    "        assignments = torch.argmin(dist, dim=1)\n",
    "        for j in range(k):\n",
    "            mask = (assignments == j)\n",
    "            if mask.sum() > 0:\n",
    "                centroids[j] = small_data[mask].mean(dim=0)\n",
    "    return centroids\n",
    "\n",
    "def analyze_image(image_path, confidence_threshold=0.001):\n",
    "    image = read_image(image_path)\n",
    "    img_tensor = image.float() / 255.0\n",
    "    img_np = img_tensor.permute(1, 2, 0).numpy()\n",
    "    height, width = img_np.shape[:2]\n",
    "    min_area = confidence_threshold * height * width  # Threshold as fraction of image size\n",
    "    img_flat = img_np.reshape(-1, 3)\n",
    "    # Downsample for faster clustering\n",
    "    small_size = (height // 4, width // 4)\n",
    "    resize = Resize(small_size)\n",
    "    small_img = resize(img_tensor)\n",
    "    small_img_np = small_img.permute(1, 2, 0).numpy()\n",
    "    small_data = torch.from_numpy(small_img_np.reshape(-1, 3)).float()\n",
    "    centroids = get_centroids(small_data)\n",
    "    data = torch.from_numpy(img_flat).float()\n",
    "    dist = torch.cdist(data, centroids)\n",
    "    assignments = torch.argmin(dist, dim=1)\n",
    "    labels = assignments.reshape(height, width).numpy()\n",
    "    all_boxes = []\n",
    "    all_box_colors = []\n",
    "    all_centers = []\n",
    "    for lab in np.unique(labels):\n",
    "        mask = (labels == lab)\n",
    "        component_labels, num_components = ndimage.label(mask)\n",
    "        for comp in range(1, num_components + 1):\n",
    "            positions = np.argwhere(component_labels == comp)\n",
    "            if len(positions) < min_area:\n",
    "                continue\n",
    "            min_y, min_x = positions.min(axis=0)\n",
    "            max_y, max_x = positions.max(axis=0)\n",
    "            box = torch.tensor([min_x, min_y, max_x, max_y])\n",
    "            all_boxes.append(box)\n",
    "            color = tuple(int(c.item() * 255) for c in centroids[lab])\n",
    "            all_box_colors.append(color)\n",
    "            # Center for patterns\n",
    "            center_x = ((min_x + max_x) / 2) / width\n",
    "            center_y = ((min_y + max_y) / 2) / height\n",
    "            all_centers.append((center_x, center_y))\n",
    "    if not all_boxes:\n",
    "        print(f\"No color shapes detected in {image_path}\")\n",
    "        return None\n",
    "    boxes = torch.stack(all_boxes)\n",
    "    image_with_boxes = draw_bounding_boxes(image, boxes, colors=all_box_colors, width=3)\n",
    "    output_path = image_path.replace('.jpg', '_color_boxes.jpg')  # Adjust extension if needed\n",
    "    save_image(image_with_boxes.float() / 255, output_path)\n",
    "    print(f\"Saved boxed image to {output_path}\")\n",
    "    return all_centers\n",
    "\n",
    "# Main function to process a folder\n",
    "def analyze_folder(folder_path):\n",
    "    image_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n",
    "    all_centers = []\n",
    "\n",
    "    for file in image_files:\n",
    "        path = os.path.join(folder_path, file)\n",
    "        centers = analyze_image(path)\n",
    "        if centers:\n",
    "            all_centers.extend(centers)\n",
    "            print(f\"Centers for {file}: {centers}\")\n",
    "\n",
    "    if all_centers:\n",
    "        # Basic pattern analysis\n",
    "        avg_x = sum(c[0] for c in all_centers) / len(all_centers)\n",
    "        avg_y = sum(c[1] for c in all_centers) / len(all_centers)\n",
    "        print(f\"\\nOverall patterns across {len(image_files)} images:\")\n",
    "        print(f\"- Average color shape center: x={avg_x:.2f} (0=left, 1=right), y={avg_y:.2f} (0=top, 1=bottom)\")\n",
    "        if avg_x < 0.4:\n",
    "            print(\"- Tendency to place color shapes on the left.\")\n",
    "        elif avg_x > 0.6:\n",
    "            print(\"- Tendency to place color shapes on the right.\")\n",
    "        else:\n",
    "            print(\"- Color shapes often centered horizontally.\")\n",
    "        if avg_y < 0.4:\n",
    "            print(\"- Color shapes often in the upper part of the frame.\")\n",
    "        elif avg_y > 0.6:\n",
    "            print(\"- Color shapes often in the lower part of the frame.\")\n",
    "        else:\n",
    "            print(\"- Color shapes often centered vertically.\")\n",
    "\n",
    "\n",
    "folder_path = \"/content/\"  # Change to your folder\n",
    "analyze_folder(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
