{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install gdown contextily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import xml.etree.ElementTree as ET\n",
    "import contextily as ctx\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense, Embedding, Concatenate, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow import feature_column as fc\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from geopy.distance import geodesic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "ROUTE_GPX = 'route.gpx'\n",
    "ROUTE_GPX_URL = 'https://drive.google.com/file/d/1-kx84-fNAOuDWSdVEYU_DjP4Om0yHnaD/view?usp=sharing'\n",
    "gdown.download(ROUTE_GPX_URL, ROUTE_GPX, quiet=True, fuzzy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_gpx(filepath):\n",
    "    root = ET.parse(filepath).getroot()\n",
    "\n",
    "    ns = '{http://www.topografix.com/GPX/1/1}'\n",
    "    data = {\n",
    "        'time': [], 'lat': [], 'lon': [],\n",
    "    }\n",
    "\n",
    "    for trk in root.findall(f\".//{ns}trk\"):\n",
    "        for trkseg in trk.findall(f\"{ns}trkseg\"):\n",
    "            for trkpt in trkseg.findall(f\"{ns}trkpt\"):\n",
    "                data['lat'].append(float(trkpt.get('lat')))\n",
    "                data['lon'].append(float(trkpt.get('lon')))\n",
    "\n",
    "                time_elem = trkpt.find(f\"{ns}time\")\n",
    "                data['time'].append(time_elem.text if time_elem is not None else None)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "df = parse_gpx(ROUTE_GPX)\n",
    "\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "df['time'] = df['time'].dt.tz_convert('UTC')\n",
    "df['time'] = (df['time'] - pd.Timestamp(\"1970-01-01\", tz='UTC')) // pd.Timedelta('1s')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bounding_box(df):\n",
    "    min_lat = df['lat'].min()\n",
    "    max_lat = df['lat'].max()\n",
    "    min_lon = df['lon'].min()\n",
    "    max_lon = df['lon'].max()\n",
    "    bottom_left = (min_lat, min_lon)\n",
    "    top_right = (max_lat, max_lon)\n",
    "    return (bottom_left, top_right)\n",
    "\n",
    "bottom_left, top_right = get_bounding_box(df)\n",
    "print(bottom_left, top_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define how many buckets you want for each dimension\n",
    "num_lat_buckets = 100\n",
    "num_lon_buckets = 100\n",
    "\n",
    "bottom_left_lat, bottom_left_lon = bottom_left\n",
    "top_right_lat, top_right_lon = top_right\n",
    "\n",
    "lat_range = top_right_lat - bottom_left_lat\n",
    "lon_range = top_right_lon - bottom_left_lon\n",
    "\n",
    "boundaries_latitude = [bottom_left_lat + (i * (lat_range / num_lat_buckets)) for i in range(1, num_lat_buckets)]\n",
    "boundaries_longitude = [bottom_left_lon + (i * (lon_range / num_lon_buckets)) for i in range(1, num_lon_buckets)]\n",
    "\n",
    "print(boundaries_latitude)\n",
    "print(boundaries_longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define time input\n",
    "time_input = Input(shape=(1,), name=\"time\")\n",
    "time_normalized = tf.keras.layers.Normalization()(time_input)\n",
    "\n",
    "# Define latitude and longitude inputs\n",
    "latitude_input = Input(shape=(1,), name=\"latitude\")\n",
    "longitude_input = Input(shape=(1,), name=\"longitude\")\n",
    "\n",
    "# Apply normalization and discretization (bucketizing)\n",
    "latitude_normalized = tf.keras.layers.Normalization()(latitude_input)\n",
    "longitude_normalized = tf.keras.layers.Normalization()(longitude_input)\n",
    "\n",
    "latitude_bucketized = tf.keras.layers.Discretization(bin_boundaries=boundaries_latitude)(latitude_normalized)\n",
    "longitude_bucketized = tf.keras.layers.Discretization(bin_boundaries=boundaries_longitude)(longitude_normalized)\n",
    "\n",
    "# Concatenate the inputs\n",
    "concatenated_inputs = tf.keras.layers.Concatenate()([time_normalized, latitude_bucketized, longitude_bucketized])\n",
    "\n",
    "# Create a model using the Functional API\n",
    "x = Dense(128, activation='relu')(concatenated_inputs)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(32, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "# Output layer for predicting latitude and longitude\n",
    "output = Dense(2, activation='linear', name='predicted_coordinates')(x)\n",
    "\n",
    "def scale_output(x):\n",
    "    min_lat, min_lon = bottom_left\n",
    "    max_lat, max_lon = top_right\n",
    "    lat = (x[:, 0] + 1) / 2 * (max_lat - min_lat) + min_lat  # Scaling from [-1, 1] to [min_lat, max_lat]\n",
    "    lon = (x[:, 1] + 1) / 2 * (max_lon - min_lon) + min_lon  # Scaling from [-1, 1] to [min_lon, max_lon]\n",
    "    return tf.stack([lat, lon], axis=1)\n",
    "\n",
    "# Apply custom scaling layer\n",
    "scaled_output = tf.keras.layers.Lambda(scale_output)(output)\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=[time_input, latitude_input, longitude_input], outputs=scaled_output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['time', 'lat', 'lon']]\n",
    "y = df[['lat', 'lon']]\n",
    "\n",
    "# Split into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_split = {\n",
    "    'latitude': X_train['lat'].values,\n",
    "    'longitude': X_train['lon'].values,\n",
    "    'time': X_train['time'].values,\n",
    "}\n",
    "X_val_split = {\n",
    "    'latitude': X_val['lat'].values,\n",
    "    'longitude': X_val['lon'].values,\n",
    "    'time': X_val['time'].values,\n",
    "}\n",
    "\n",
    "# Ensure y_train and y_val are structured correctly\n",
    "y_train_combined = np.column_stack((y_train['lat'].values, y_train['lon'].values))\n",
    "y_val_combined = np.column_stack((y_val['lat'].values, y_val['lon'].values))\n",
    "\n",
    "# Fit the model\n",
    "model.fit(\n",
    "    x=X_train_split,\n",
    "    y=y_train_combined,  # Use combined array\n",
    "    validation_data=(X_val_split, y_val_combined),  # Use combined array\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[EarlyStopping(patience=10, restore_best_weights=True)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    return geodesic((lat1, lon1), (lat2, lon2)).meters\n",
    "\n",
    "def predict_over_distance(model, initial_point, max_distance=1000.0, time_step=60):\n",
    "    predictions = []\n",
    "    cumulative_distance = 0.0\n",
    "    current_point = initial_point\n",
    "\n",
    "    while cumulative_distance < max_distance:\n",
    "        predicted_point = model.predict(x={\n",
    "            'latitude': np.array([[current_point[1]]]),\n",
    "            'longitude': np.array([[current_point[2]]]),\n",
    "            'time': np.array([[current_point[0]]]),\n",
    "        }, verbose=0)[0]\n",
    "        print(predicted_point)\n",
    "\n",
    "        # Calculate distance with the denormalized coordinates\n",
    "        dist = haversine_distance(current_point[1], current_point[2], predicted_point[0], predicted_point[1])\n",
    "        cumulative_distance += dist\n",
    "\n",
    "        print(f\"Current: ({current_point[1]:.6f}, {current_point[2]:.6f}) | \"\n",
    "              f\"Predicted: ({predicted_point[0]:.6f}, {predicted_point[1]:.6f}) | \"\n",
    "              f\"Distance : {dist:.2f} m\")\n",
    "\n",
    "        # Update time, keeping it in its original scale\n",
    "        new_time = current_point[0] + time_step\n",
    "\n",
    "        # Store prediction in original scale\n",
    "        predictions.append([new_time, predicted_point[0], predicted_point[1]])\n",
    "\n",
    "        # Update current point for next iteration, but time should remain in original scale\n",
    "        current_point = [new_time, predicted_point[0], predicted_point[1]]\n",
    "\n",
    "        # Break if cumulative distance meets or exceeds the target\n",
    "        if cumulative_distance >= max_distance:\n",
    "            break\n",
    "\n",
    "    return predictions\n",
    "\n",
    "initial_time = df['time'].iloc[-1]\n",
    "initial_lat = df['lat'].iloc[-1]\n",
    "initial_lon = df['lon'].iloc[-1]\n",
    "initial_point = [initial_time, initial_lat, initial_lon]\n",
    "\n",
    "predicted_path = predict_over_distance(model, initial_point)\n",
    "\n",
    "# Convert predictions to a DataFrame for easy viewing\n",
    "predicted_df = pd.DataFrame(predicted_path, columns=['time', 'lat', 'lon'])\n",
    "print(predicted_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
